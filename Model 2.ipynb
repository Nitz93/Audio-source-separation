{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "animal-avatar",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the packages we'll use\n",
    "\n",
    "import numpy as np\n",
    "import os, glob, csv\n",
    "\n",
    "# librosa is a widely-used audio processing library\n",
    "import librosa\n",
    "\n",
    "import sklearn\n",
    "import scipy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torchvision import transforms\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset as BaseDataset\n",
    "\n",
    "# for plotting\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import math\n",
    "\n",
    "# for accuracy and confusion matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "# for data normalization\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# use GPU if available, otherwise, use cpu\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eight-offering",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mir_eval  \n",
    "import museval.metrics as metrics\n",
    "import numpy as np\n",
    "from scipy.io import wavfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "authentic-civilization",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = os.listdir('./musdb18hq/train')\n",
    "ids = [x for x in ids if not x.startswith('.')]\n",
    "filenames =[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reasonable-insulin",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_audio(path, seq):\n",
    "    example_audio, sr = librosa.load(path, sr=44100, offset=seq*30.0, duration=60.0)\n",
    "    return example_audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "serial-guinea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mixture(filename,seq, win_len=0.05, hop_len=0.0125, n_mels=64):\n",
    "    audio, sr = librosa.load(\"%s.wav\" % (filename), sr=44100, offset=seq*30.0, duration=60.0)\n",
    "    win_len = int(win_len*sr)\n",
    "    hop_len = int(hop_len*sr)\n",
    "    spec, phase = librosa.magphase(librosa.stft(audio, n_fft=win_len,hop_length=hop_len,window='hann',center='True'))\n",
    "    spec = spec.transpose((1,0))\n",
    "    phase = phase.transpose((1,0))\n",
    "    if(len(spec) < 4802):\n",
    "        spec = np.pad(spec, ((4802-len(spec),0),(0,0)))\n",
    "        phase = np.pad(phase, ((4802-len(phase),0),(0,0)))\n",
    "    spec = spec[:4802]\n",
    "    phase = phase[:4802]\n",
    "    spec = spec.transpose((1,0))\n",
    "    phase = phase.transpose((1,0))\n",
    "    return spec, phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vietnamese-michigan",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_segments(filename,seq, win_len=0.05, hop_len=0.0125, n_mels=64):\n",
    "    audio, sr = librosa.load(\"%s.wav\" % (filename), sr=44100,offset=seq*30.0,duration=60.0)\n",
    "    win_len = int(win_len*sr)\n",
    "    hop_len = int(hop_len*sr)\n",
    "    spec, phase = librosa.magphase(librosa.stft(audio, n_fft=win_len,hop_length=hop_len,window='hann',center='True'))\n",
    "    spec = spec.transpose((1,0))\n",
    "    if(len(spec) < 4802):\n",
    "        spec = np.pad(spec, ((4802-len(spec),0),(0,0)))\n",
    "    spec = spec[:4802]\n",
    "    spec = spec.transpose((1,0))\n",
    "    return spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "naked-insulin",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(BaseDataset):\n",
    "    \n",
    "    def __init__(self,ids, seq, path='./musdb18hq/train', transforms=None):\n",
    "            \n",
    "        self.ids = ids\n",
    "        self.ids = [x for x in self.ids if not x.startswith('.')]\n",
    "        self.path = path+'/'\n",
    "        self.seq = seq\n",
    "        \n",
    "    def __getitem__(self, i):\n",
    "        \n",
    "        # read data\n",
    "        mixture_path = '/mixture'\n",
    "        bass_path = '/bass'\n",
    "        vocals_path = '/vocals'\n",
    "        drums_path = '/drums'\n",
    "        others_path = '/other'\n",
    "        mixture, m_phase = load_mixture(self.path+self.ids[i]+mixture_path,seq = self.seq)\n",
    "        #phase = torch.load(mixture_path+self.list[index]+'_p')\n",
    "        bass = load_segments(self.path+self.ids[i]+bass_path, seq = self.seq)\n",
    "        vocals = load_segments(self.path+self.ids[i]+vocals_path, seq = self.seq)\n",
    "        drums = load_segments(self.path+self.ids[i]+drums_path, seq = self.seq)\n",
    "        others = load_segments(self.path+self.ids[i]+others_path, seq = self.seq)\n",
    "            \n",
    "        return mixture, m_phase, bass, vocals, drums, others\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "romantic-boston",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self,input_shape=[1103, 4803]):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=10,\n",
    "                               kernel_size=(1102,1), stride=1)\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=(2,2),stride=2)\n",
    "        self.conv2 = nn.Conv2d(10,20, kernel_size=(1,25))\n",
    "        \"\"\"self.maxpool2 = nn.MaxPool2d(kernel_size=(2,2),stride=2)\n",
    "        self.conv3 = nn.Conv2d(20,30, kernel_size=(5,15))\n",
    "        self.deconv3_1 = nn.ConvTranspose2d(30, 20, kernel_size=(5,15))\n",
    "        self.deconv3_2 = nn.ConvTranspose2d(30, 20, kernel_size=(5,15))\n",
    "        self.deconv3_3 = nn.ConvTranspose2d(30, 20, kernel_size=(5,15))\n",
    "        self.deconv3_4 = nn.ConvTranspose2d(30, 20, kernel_size=(5,15))\n",
    "        self.upsample1 = nn.Upsample(scale_factor=2, mode='nearest')\"\"\"\n",
    "        self.deconv2_1 = nn.ConvTranspose2d(20, 10, kernel_size=(1,25), stride=1)\n",
    "        self.deconv2_2 = nn.ConvTranspose2d(20, 10, kernel_size=(1,25), stride=1)\n",
    "        self.deconv2_3 = nn.ConvTranspose2d(20, 10, kernel_size=(1,25), stride=1)\n",
    "        self.deconv2_4 = nn.ConvTranspose2d(20, 10, kernel_size=(1,25), stride=1)\n",
    "        self.upsample2 = nn.Upsample(scale_factor=2, mode='nearest')\n",
    "        self.deconv1_1 = nn.ConvTranspose2d(10, 5, kernel_size=(1102,1), stride=1)\n",
    "        self.deconv1_2 = nn.ConvTranspose2d(10, 5, kernel_size=(1102,1), stride=1)\n",
    "        self.deconv1_3 = nn.ConvTranspose2d(10, 5, kernel_size=(1102,1), stride=1)\n",
    "        self.deconv1_4 = nn.ConvTranspose2d(10, 5, kernel_size=(1102,1), stride=1)\n",
    "        self.conv = nn.Conv2d(5,1, kernel_size=(1,1))\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        (_, time_len, mel_bins) = x.shape\n",
    "\n",
    "        x = x.view(-1, 1, time_len, mel_bins)\n",
    "        x = F.relu(self.maxpool1(self.conv1(x)))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        \"\"\"x = F.relu(self.conv3(x))\n",
    "        \n",
    "        x1 = self.deconv3_1(x)\n",
    "        x2 = self.deconv3_2(x)\n",
    "        x3 = self.deconv3_3(x)\n",
    "        x4 = self.deconv3_4(x)\n",
    "        \n",
    "        x1 = self.dropout(x1)\n",
    "        x2 = self.dropout(x2)\n",
    "        x3 = self.dropout(x3)\n",
    "        x4 = self.dropout(x4)\n",
    "        \n",
    "        x1 = self.upsample1(x1)\n",
    "        x2 = self.upsample1(x2)\n",
    "        x3 = self.upsample1(x3)\n",
    "        x4 = self.upsample1(x4)\"\"\"\n",
    "        \n",
    "\n",
    "        x1 = self.deconv2_1(x)\n",
    "        x2 = self.deconv2_2(x)\n",
    "        x3 = self.deconv2_3(x)\n",
    "        x4 = self.deconv2_4(x)\n",
    "        \n",
    "        x1 = self.dropout(x1)\n",
    "        x2 = self.dropout(x2)\n",
    "        x3 = self.dropout(x3)\n",
    "        x4 = self.dropout(x4)\n",
    "        \n",
    "        x1 = self.upsample2(x1)\n",
    "        x2 = self.upsample2(x2)\n",
    "        x3 = self.upsample2(x3)\n",
    "        x4 = self.upsample2(x4)\n",
    "                                \n",
    "        x1 = self.deconv1_1(x1)\n",
    "        x2 = self.deconv1_2(x2)\n",
    "        x3 = self.deconv1_3(x3)\n",
    "        x4 = self.deconv1_4(x4)\n",
    "        \n",
    "        x1 = self.dropout(x1)\n",
    "        x2 = self.dropout(x2)\n",
    "        x3 = self.dropout(x3)\n",
    "        x4 = self.dropout(x4)\n",
    "        \n",
    "        x1 = F.relu(self.conv(x1))\n",
    "        x2 = F.relu(self.conv(x2))\n",
    "        x3 = F.relu(self.conv(x3))\n",
    "        x4 = F.relu(self.conv(x4))\n",
    "\n",
    "        return x1, x2, x3, x4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "robust-lotus",
   "metadata": {},
   "outputs": [],
   "source": [
    "netb = ConvNet([1103, 4802])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extraordinary-radius",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "summary(netb, (1103, 4802))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "developmental-slope",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_var_path= \"../Processed/\"\n",
    "if not os.path.exists('Weights'):\n",
    "    os.makedirs('Weights')\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "#--------------------------\n",
    "class Average(object):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.sum += val\n",
    "        self.count += n\n",
    "\n",
    "    #property\n",
    "    def avg(self):\n",
    "        return self.sum / self.count\n",
    "#------------------------------\n",
    "# import csv\n",
    "writer = SummaryWriter()\n",
    "#----------------------------------------\n",
    "\n",
    "inp_size = [1103, 4802]\n",
    "alpha = 0.005\n",
    "beta = 0.05\n",
    "beta_vocals = 0.08\n",
    "batch_size = 15\n",
    "num_epochs = 50\n",
    "cur_epoch =47\n",
    "\n",
    "class MixedSquaredError(nn.Module):\n",
    "    def __init__(self, weight=None, size_average=True):\n",
    "        super(MixedSquaredError, self).__init__()\n",
    "\n",
    "    def forward(self, pred_bass,pred_vocals,pred_drums,pred_others, gt_bass,gt_vocals,gt_drums, gt_others):\n",
    "\n",
    "\n",
    "        L_sq = torch.sum((pred_bass-gt_bass).pow(2)) + torch.sum((pred_vocals-gt_vocals).pow(2)) + torch.sum((pred_drums-gt_drums).pow(2)) + torch.sum((pred_others-gt_others).pow(2))\n",
    "        L_other = torch.sum((pred_bass-gt_others).pow(2)) + torch.sum((pred_drums-gt_others).pow(2))\n",
    "        #+ torch.sum((pred_vocals-gt_others).pow(2))\n",
    "        L_othervocals = torch.sum((pred_vocals - gt_others).pow(2))\n",
    "        L_diff = torch.sum((pred_bass-pred_vocals).pow(2)) + torch.sum((pred_bass-pred_drums).pow(2)) + torch.sum((pred_vocals-pred_drums).pow(2))\n",
    "\n",
    "        return (L_sq- alpha*L_diff - beta*L_other - beta_vocals*L_othervocals)\n",
    "\n",
    "def TimeFreqMasking(bass,vocals,drums,others,cuda=0):\n",
    "    den = torch.abs(bass) + torch.abs(vocals) + torch.abs(drums) + torch.abs(others)\n",
    "    if(cuda):\n",
    "        den = den + 10e-8*torch.cuda.FloatTensor(bass.size()).normal_()\n",
    "    else:\n",
    "        den = den + 10e-8*torch.FloatTensor(bass.size()).normal_()\n",
    "        \n",
    "    \n",
    "    bass = torch.abs(bass)/den\n",
    "    vocals = torch.abs(vocals)/den\n",
    "    drums = torch.abs(drums)/den\n",
    "    others = torch.abs(others)/den\n",
    "    \n",
    "    return bass,vocals,drums,others\n",
    "\n",
    "\n",
    "def train():\n",
    "    \n",
    "    cuda = torch.cuda.is_available()\n",
    "    net = ConvNet(inp_size)\n",
    "    net.load_state_dict(torch.load('Weights/Weights_9_128225384.0.pth'))\n",
    "    criterion = MixedSquaredError()\n",
    "    if cuda:\n",
    "        net = net.cuda()\n",
    "        criterion = criterion.cuda()\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr = 0.001)\n",
    "\n",
    "    print(\"preparing training data ...\")\n",
    "    \n",
    "    ids = os.listdir('./musdb18hq/train')\n",
    "    ids = [x for x in ids if not x.startswith('.')]\n",
    "    filenames = []\n",
    "    \n",
    "    val_ids = os.listdir('./musdb18hq/val')\n",
    "    val_ids = [x for x in val_ids if not x.startswith('.')]\n",
    "    val_filenames = []\n",
    "    for i in val_ids:\n",
    "        # load an example audio file, converting the data to mel spectrogram\n",
    "        path = './musdb18hq/val/'+i+'/mixture.wav'\n",
    "        example_audio = load_audio(path, seq = 0)\n",
    "        if example_audio.shape[0] > 0:\n",
    "            val_filenames.append(i)\n",
    "    val_set = Dataset(ids = val_filenames, seq = 0, path='./musdb18hq/val',transforms = None)\n",
    "    val_loader = DataLoader(val_set, batch_size=batch_size,shuffle=False)\n",
    "    \n",
    "    \n",
    "    for epoch in range(cur_epoch, num_epochs+1):\n",
    "        batch = 0\n",
    "        filenames = []\n",
    "        for i in ids:\n",
    "            # load an example audio file, converting the data to mel spectrogram\n",
    "            path = './musdb18hq/train/'+i+'/mixture.wav'\n",
    "            example_audio = load_audio(path, seq = 0)\n",
    "            if example_audio.shape[0] > 0:\n",
    "                filenames.append(i)\n",
    "        print(len(filenames))\n",
    "        train_set = Dataset(ids = filenames, seq = 0, transforms = None)\n",
    "        train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "        print(\"done ...\")\n",
    "\n",
    "        train_loss = Average()       \n",
    "\n",
    "        net.train()\n",
    "        for i, (inp,phase, gt_bass,gt_vocals,gt_drums,gt_others) in enumerate(train_loader):\n",
    "            print(f'epoch {epoch+1}.... batch {batch+i+1}')\n",
    "            inp = torch.FloatTensor(inp)\n",
    "            mean = torch.mean(inp)\n",
    "            std = torch.std(inp)\n",
    "            inp_n = (inp-mean)/std\n",
    "            inp_n = torch.FloatTensor(inp_n)\n",
    "            gt_bass = torch.FloatTensor(gt_bass)\n",
    "            gt_vocals = torch.FloatTensor(gt_vocals)\n",
    "            gt_drums = torch.FloatTensor(gt_drums)\n",
    "            gt_others= torch.FloatTensor(gt_others)\n",
    "            if cuda:\n",
    "                inp = inp.cuda()\n",
    "                inp_n = inp_n.cuda()\n",
    "                gt_bass = gt_bass.cuda()\n",
    "                gt_vocals = gt_vocals.cuda()\n",
    "                gt_drums = gt_drums.cuda()\n",
    "                gt_others= gt_others.cuda()\n",
    "            optimizer.zero_grad()\n",
    "            bass, vocals, drums, others = net(torch.FloatTensor(inp_n))\n",
    "            mask_bass,mask_vocals,mask_drums,mask_others = TimeFreqMasking(bass, vocals, drums, others,cuda)\n",
    "\n",
    "            pred_drums=inp*np.squeeze(mask_drums)\n",
    "            pred_vocals=inp*np.squeeze(mask_vocals)\n",
    "            pred_bass=inp*np.squeeze(mask_bass)\n",
    "            pred_others=inp*np.squeeze(mask_others)\n",
    "\n",
    "            loss = criterion(pred_bass,pred_vocals,pred_drums,pred_others, gt_bass,gt_vocals,gt_drums,gt_others)\n",
    "            print(loss.item())\n",
    "            writer.add_scalar('Train Loss',loss,epoch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss.update(loss.item(), inp.size(0))\n",
    "            \n",
    "            \n",
    "        #..........\n",
    "        filenames = []\n",
    "        print(len(filenames))\n",
    "        for i in ids:\n",
    "            # load an example audio file, converting the data to mel spectrogram\n",
    "            examplefkey = './musdb18hq/train/'+i+'/mixture.wav'\n",
    "            example_audio = load_audio(examplefkey, seq = 1)\n",
    "            if example_audio.shape[0] > 0:\n",
    "                filenames.append(i)\n",
    "        print(len(filenames))\n",
    "        train_set = Dataset(ids = filenames, seq = 1, transforms = None)\n",
    "        train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "        print(\"done ...\")\n",
    "\n",
    "        for i, (inp,phase, gt_bass,gt_vocals,gt_drums,gt_others) in enumerate(train_loader):\n",
    "            print(f'epoch {epoch+1}.... batch {batch+i+1}')\n",
    "            inp = torch.FloatTensor(inp)\n",
    "            mean = torch.mean(inp)\n",
    "            std = torch.std(inp)\n",
    "            inp_n = (inp-mean)/std\n",
    "            inp_n = torch.FloatTensor(inp_n)\n",
    "            gt_bass = torch.FloatTensor(gt_bass)\n",
    "            gt_vocals = torch.FloatTensor(gt_vocals)\n",
    "            gt_drums = torch.FloatTensor(gt_drums)\n",
    "            gt_others= torch.FloatTensor(gt_others)\n",
    "            if cuda:\n",
    "                inp = inp.cuda()\n",
    "                inp_n = inp_n.cuda()\n",
    "                gt_bass = gt_bass.cuda()\n",
    "                gt_vocals = gt_vocals.cuda()\n",
    "                gt_drums = gt_drums.cuda()\n",
    "                gt_others= gt_others.cuda()\n",
    "            optimizer.zero_grad()\n",
    "            bass, vocals, drums, others = net(torch.FloatTensor(inp_n))\n",
    "            mask_bass,mask_vocals,mask_drums,mask_others = TimeFreqMasking(bass, vocals, drums, others,cuda)\n",
    "\n",
    "            pred_drums=inp*np.squeeze(mask_drums)\n",
    "            pred_vocals=inp*np.squeeze(mask_vocals)\n",
    "            pred_bass=inp*np.squeeze(mask_bass)\n",
    "            pred_others=inp*np.squeeze(mask_others)\n",
    "\n",
    "            loss = criterion(pred_bass,pred_vocals,pred_drums,pred_others, gt_bass,gt_vocals,gt_drums,gt_others)\n",
    "            writer.add_scalar('Train Loss',loss,epoch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss.update(loss.item(), inp.size(0))\n",
    "        \n",
    "        #..................\n",
    "        filenames = []\n",
    "        for i in ids:\n",
    "            # load an example audio file, converting the data to mel spectrogram\n",
    "            examplefkey = './musdb18hq/train/'+i+'/mixture.wav'\n",
    "            example_audio = load_audio(examplefkey, seq = 2)\n",
    "            if example_audio.shape[0] > 0:\n",
    "                filenames.append(i)\n",
    "        print(len(filenames))\n",
    "        train_set = Dataset(ids = filenames, seq = 2, transforms = None)\n",
    "        train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "        print(\"done ...\")\n",
    "\n",
    "        for i, (inp,phase, gt_bass,gt_vocals,gt_drums,gt_others) in enumerate(train_loader):\n",
    "            print(f'epoch {epoch+1}.... batch {batch+i+1}')\n",
    "            inp = torch.FloatTensor(inp)\n",
    "            mean = torch.mean(inp)\n",
    "            std = torch.std(inp)\n",
    "            inp_n = (inp-mean)/std\n",
    "            inp_n = torch.FloatTensor(inp_n)\n",
    "            gt_bass = torch.FloatTensor(gt_bass)\n",
    "            gt_vocals = torch.FloatTensor(gt_vocals)\n",
    "            gt_drums = torch.FloatTensor(gt_drums)\n",
    "            gt_others= torch.FloatTensor(gt_others)\n",
    "            if cuda:\n",
    "                inp = inp.cuda()\n",
    "                inp_n = inp_n.cuda()\n",
    "                gt_bass = gt_bass.cuda()\n",
    "                gt_vocals = gt_vocals.cuda()\n",
    "                gt_drums = gt_drums.cuda()\n",
    "                gt_others= gt_others.cuda()\n",
    "            optimizer.zero_grad()\n",
    "            bass, vocals, drums, others = net(torch.FloatTensor(inp_n))\n",
    "            mask_bass,mask_vocals,mask_drums,mask_others = TimeFreqMasking(bass, vocals, drums, others,cuda)\n",
    "\n",
    "            pred_drums=inp*np.squeeze(mask_drums)\n",
    "            pred_vocals=inp*np.squeeze(mask_vocals)\n",
    "            pred_bass=inp*np.squeeze(mask_bass)\n",
    "            pred_others=inp*np.squeeze(mask_others)\n",
    "\n",
    "            loss = criterion(pred_bass,pred_vocals,pred_drums,pred_others, gt_bass,gt_vocals,gt_drums,gt_others)\n",
    "            writer.add_scalar('Train Loss',loss,epoch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss.update(loss.item(), inp.size(0))\n",
    "        \n",
    "        #..............\n",
    "        filenames = []\n",
    "        for i in ids:\n",
    "            # load an example audio file, converting the data to mel spectrogram\n",
    "            examplefkey = './musdb18hq/train/'+i+'/mixture.wav'\n",
    "            example_audio = load_audio(examplefkey, seq = 3)\n",
    "            if example_audio.shape[0] > 0:\n",
    "                filenames.append(i)\n",
    "        print(len(filenames))\n",
    "        train_set = Dataset(ids = filenames, seq = 3, transforms = None)\n",
    "        train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "        print(\"done ...\")\n",
    "\n",
    "        for i, (inp,phase, gt_bass,gt_vocals,gt_drums,gt_others) in enumerate(train_loader):\n",
    "            print(f'epoch {epoch+1}.... batch {batch+i+1}')\n",
    "            inp = torch.FloatTensor(inp)\n",
    "            mean = torch.mean(inp)\n",
    "            std = torch.std(inp)\n",
    "            inp_n = (inp-mean)/std\n",
    "            inp_n = torch.FloatTensor(inp_n)\n",
    "            gt_bass = torch.FloatTensor(gt_bass)\n",
    "            gt_vocals = torch.FloatTensor(gt_vocals)\n",
    "            gt_drums = torch.FloatTensor(gt_drums)\n",
    "            gt_others= torch.FloatTensor(gt_others)\n",
    "            if cuda:\n",
    "                inp = inp.cuda()\n",
    "                inp_n = inp_n.cuda()\n",
    "                gt_bass = gt_bass.cuda()\n",
    "                gt_vocals = gt_vocals.cuda()\n",
    "                gt_drums = gt_drums.cuda()\n",
    "                gt_others= gt_others.cuda()\n",
    "            optimizer.zero_grad()\n",
    "            bass, vocals, drums, others = net(torch.FloatTensor(inp_n))\n",
    "            mask_bass,mask_vocals,mask_drums,mask_others = TimeFreqMasking(bass, vocals, drums, others,cuda)\n",
    "\n",
    "            pred_drums=inp*np.squeeze(mask_drums)\n",
    "            pred_vocals=inp*np.squeeze(mask_vocals)\n",
    "            pred_bass=inp*np.squeeze(mask_bass)\n",
    "            pred_others=inp*np.squeeze(mask_others)\n",
    "\n",
    "            loss = criterion(pred_bass,pred_vocals,pred_drums,pred_others, gt_bass,gt_vocals,gt_drums,gt_others)\n",
    "            writer.add_scalar('Train Loss',loss,epoch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss.update(loss.item(), inp.size(0))\n",
    "        \n",
    "        #......................\n",
    "        filenames = []\n",
    "        for i in ids:\n",
    "            # load an example audio file, converting the data to mel spectrogram\n",
    "            examplefkey = './musdb18hq/train/'+i+'/mixture.wav'\n",
    "            example_audio = load_audio(examplefkey, seq = 4)\n",
    "            if example_audio.shape[0] > 0:\n",
    "                filenames.append(i)\n",
    "        print(len(filenames))\n",
    "        train_set = Dataset(ids = filenames, seq = 4, transforms = None)\n",
    "        train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "        print(\"done ...\")\n",
    "\n",
    "        for i, (inp,phase, gt_bass,gt_vocals,gt_drums,gt_others) in enumerate(train_loader):\n",
    "            print(f'epoch {epoch+1}.... batch {batch+i+1}')\n",
    "            inp = torch.FloatTensor(inp)\n",
    "            mean = torch.mean(inp)\n",
    "            std = torch.std(inp)\n",
    "            inp_n = (inp-mean)/std\n",
    "            inp_n = torch.FloatTensor(inp_n)\n",
    "            gt_bass = torch.FloatTensor(gt_bass)\n",
    "            gt_vocals = torch.FloatTensor(gt_vocals)\n",
    "            gt_drums = torch.FloatTensor(gt_drums)\n",
    "            gt_others= torch.FloatTensor(gt_others)\n",
    "            if cuda:\n",
    "                inp = inp.cuda()\n",
    "                inp_n = inp_n.cuda()\n",
    "                gt_bass = gt_bass.cuda()\n",
    "                gt_vocals = gt_vocals.cuda()\n",
    "                gt_drums = gt_drums.cuda()\n",
    "                gt_others= gt_others.cuda()\n",
    "            optimizer.zero_grad()\n",
    "            bass, vocals, drums, others = net(torch.FloatTensor(inp_n))\n",
    "            mask_bass,mask_vocals,mask_drums,mask_others = TimeFreqMasking(bass, vocals, drums, others,cuda)\n",
    "\n",
    "            pred_drums=inp*np.squeeze(mask_drums)\n",
    "            pred_vocals=inp*np.squeeze(mask_vocals)\n",
    "            pred_bass=inp*np.squeeze(mask_bass)\n",
    "            pred_others=inp*np.squeeze(mask_others)\n",
    "\n",
    "            loss = criterion(pred_bass,pred_vocals,pred_drums,pred_others, gt_bass,gt_vocals,gt_drums,gt_others)\n",
    "            writer.add_scalar('Train Loss',loss,epoch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss.update(loss.item(), inp.size(0))\n",
    "        #.........................\n",
    "        \n",
    "        filenames = []\n",
    "        for i in ids:\n",
    "            # load an example audio file, converting the data to mel spectrogram\n",
    "            examplefkey = './musdb18hq/train/'+i+'/mixture.wav'\n",
    "            example_audio = load_audio(examplefkey, seq = 5)\n",
    "            if example_audio.shape[0] > 0:\n",
    "                filenames.append(i)\n",
    "        print(len(filenames))\n",
    "        train_set = Dataset(ids = filenames, seq = 5, transforms = None)\n",
    "        train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "        print(\"done ...\")\n",
    "\n",
    "        for i, (inp,phase, gt_bass,gt_vocals,gt_drums,gt_others) in enumerate(train_loader):\n",
    "            print(f'epoch {epoch+1}.... batch {batch+i+1}')\n",
    "            inp = torch.FloatTensor(inp)\n",
    "            mean = torch.mean(inp)\n",
    "            std = torch.std(inp)\n",
    "            inp_n = (inp-mean)/std\n",
    "            inp_n = torch.FloatTensor(inp_n)\n",
    "            gt_bass = torch.FloatTensor(gt_bass)\n",
    "            gt_vocals = torch.FloatTensor(gt_vocals)\n",
    "            gt_drums = torch.FloatTensor(gt_drums)\n",
    "            gt_others= torch.FloatTensor(gt_others)\n",
    "            if cuda:\n",
    "                inp = inp.cuda()\n",
    "                inp_n = inp_n.cuda()\n",
    "                gt_bass = gt_bass.cuda()\n",
    "                gt_vocals = gt_vocals.cuda()\n",
    "                gt_drums = gt_drums.cuda()\n",
    "                gt_others= gt_others.cuda()\n",
    "            optimizer.zero_grad()\n",
    "            bass, vocals, drums, others = net(torch.FloatTensor(inp_n))\n",
    "            mask_bass,mask_vocals,mask_drums,mask_others = TimeFreqMasking(bass, vocals, drums, others,cuda)\n",
    "\n",
    "            pred_drums=inp*np.squeeze(mask_drums)\n",
    "            pred_vocals=inp*np.squeeze(mask_vocals)\n",
    "            pred_bass=inp*np.squeeze(mask_bass)\n",
    "            pred_others=inp*np.squeeze(mask_others)\n",
    "\n",
    "            loss = criterion(pred_bass,pred_vocals,pred_drums,pred_others, gt_bass,gt_vocals,gt_drums,gt_others)\n",
    "            writer.add_scalar('Train Loss',loss,epoch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss.update(loss.item(), inp.size(0))\n",
    "        #.........................\n",
    "\n",
    "        val_loss = Average()\n",
    "        net.eval()\n",
    "        for i,(val_inp,val_phase, gt_bass,gt_vocals,gt_drums,gt_others) in enumerate(val_loader):\n",
    "            \n",
    "            val_inp = torch.FloatTensor(val_inp)\n",
    "            val_mean = torch.mean(val_inp)\n",
    "            val_std = torch.std(val_inp)\n",
    "            val_inp_n = (val_inp-val_mean)/val_std\n",
    "            val_inp_n = torch.FloatTensor(val_inp_n)\n",
    "            gt_bass = torch.FloatTensor(gt_bass)\n",
    "            gt_vocals = torch.FloatTensor(gt_vocals)\n",
    "            gt_drums = torch.FloatTensor(gt_drums)\n",
    "            gt_others= torch.FloatTensor(gt_others)\n",
    "            if cuda:\n",
    "                val_inp = val_inp.cuda()\n",
    "                val_inp_n = val_inp_n.cuda()\n",
    "                gt_bass = gt_bass.cuda()\n",
    "                gt_vocals = gt_vocals.cuda()\n",
    "                gt_drums = gt_drums.cuda()\n",
    "                gt_others = gt_others.cuda()\n",
    "\n",
    "            bass, vocals, drums, others = net(val_inp_n)\n",
    "            mask_bass,mask_vocals,mask_drums,mask_others = TimeFreqMasking(bass, vocals, drums, others,cuda)\n",
    "\n",
    "            pred_drums=val_inp*np.squeeze(mask_drums)\n",
    "            pred_vocals=val_inp*np.squeeze(mask_vocals)\n",
    "            pred_bass=val_inp*np.squeeze(mask_bass)\n",
    "            pred_others=val_inp*np.squeeze(mask_others)\n",
    "            \n",
    "\n",
    "            \"\"\"if (epoch)%10==0:\n",
    "                writer.add_image('Validation Input',val_inp,epoch)\n",
    "                writer.add_image('Validation Bass GT ',gt_bass,epoch)\n",
    "                writer.add_image('Validation Bass Pred ',pred_bass,epoch)\n",
    "                writer.add_image('Validation Vocals GT ',gt_vocals,epoch)\n",
    "                writer.add_image('Validation Vocals Pred ',pred_vocals,epoch)\n",
    "                writer.add_image('Validation Drums GT ',gt_drums,epoch)\n",
    "                writer.add_image('Validation Drums Pred ',pred_drums,epoch)\n",
    "                writer.add_image('Validation Other GT ',gt_others,epoch)\n",
    "                writer.add_image('Validation Others Pred ',pred_others,epoch)\"\"\"\n",
    "\n",
    "            vloss = criterion(pred_bass,pred_vocals,pred_drums,pred_others, gt_bass,gt_vocals,gt_drums, gt_others)\n",
    "            writer.add_scalar('Validation loss',vloss,epoch)\n",
    "            val_loss.update(vloss.item(), inp.size(0))\n",
    "\n",
    "        print(\"Epoch {}, Training Loss: {}, Validation Loss: {}\".format(epoch+1, train_loss.avg(), val_loss.avg()))\n",
    "        torch.save(net.state_dict(), 'Weights/Weights_{}_{}.pth'.format(epoch+1, val_loss.avg()))\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "patent-development",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "needed-psychology",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = net = ConvNet(inp_size)\n",
    "    # net.load_state_dict(torch.load('Weights/Weights_200_3722932.6015625.pth')) #least score Weights so far\n",
    "net.load_state_dict(torch.load('Weights/Weights_49_178278304.0.pth'))\n",
    "net.eval()\n",
    "filenames = []\n",
    "ids = os.listdir('./musdb18hq/test')\n",
    "ids = [x for x in ids if not x.startswith('.')]\n",
    "filenames = []\n",
    "count = 0\n",
    "for i in ids:\n",
    "    examplefkey = './musdb18hq/test/'+i+'/mixture.wav'\n",
    "    example_audio = load_audio(examplefkey, seq = 0)\n",
    "    if example_audio.shape[0] > 0:\n",
    "        filenames.append(i)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break\n",
    "\n",
    "print(len(filenames))\n",
    "    \n",
    "for i in filenames:\n",
    "# load an example audio file, converting the data to mel spectrogram\n",
    "    total_SDR_bass = total_ISR_bass =  total_SIR_bass = total_SAR_bass =  0\n",
    "    total_SDR_drum = total_ISR_drum = total_SIR_drum = total_SAR_drum =  0\n",
    "    total_SDR_vocal = total_ISR_vocal = total_SIR_vocal = total_SAR_vocal =  0\n",
    "    total_SDR_other = total_ISR_other = total_SIR_other = total_SAR_other =  0\n",
    "    \n",
    "    examplefkey = './musdb18hq/test/'+i\n",
    "    mixture, m_phase = load_mixture(examplefkey+'/mixture',0)\n",
    "    bass_gt = load_segments(examplefkey+'/bass', seq = 0)\n",
    "    vocals_gt = load_segments(examplefkey+'/vocals', seq = 0)\n",
    "    drums_gt = load_segments(examplefkey+'/drums', seq = 0)\n",
    "    others_gt = load_segments(examplefkey+'/other', seq = 0)\n",
    "    mean = torch.mean(torch.tensor(mixture))\n",
    "    std = torch.std(torch.tensor(mixture))\n",
    "    inp_n = torch.tensor(mixture)\n",
    "    (time_len, mel_bins) = inp_n.shape\n",
    "    inp_n = inp_n.view(1,time_len, mel_bins)\n",
    "    print(inp_n.shape)\n",
    "    bass_mag, vocals_mag, drums_mag,others_mag = net(torch.tensor(inp_n))\n",
    "    print(bass_mag.shape)\n",
    "    drums_mag=torch.tensor(mixture)*np.squeeze(bass_mag)\n",
    "    vocals_mag=torch.tensor(mixture)*np.squeeze(vocals_mag)\n",
    "    bass_mag=torch.tensor(mixture)*np.squeeze(drums_mag)\n",
    "    others_mag=torch.tensor(mixture)*np.squeeze(others_mag)\n",
    "    print(bass_mag.shape)    \n",
    "    vocals = np.squeeze(vocals_mag.detach().numpy())\n",
    "        #print(vocals.shape)\n",
    "    bass = np.squeeze(bass_mag.detach().numpy())\n",
    "    drums = np.squeeze(drums_mag.detach().numpy())\n",
    "    others = np.squeeze(others_mag.detach().numpy())\n",
    "    \n",
    "    shape = drums_gt.flatten().shape\n",
    "    \n",
    "    \n",
    "    SDR_bass,ISR_bass, SIR_bass, SAR_bass, perm = mir_eval.separation.bss_eval_images(bass_gt.reshape((1, shape[0],1)), bass.reshape((1, shape[0], 1)))\n",
    "    total_SDR_bass += abs(SDR_bass[0])\n",
    "    total_ISR_bass += abs(ISR_bass[0])\n",
    "    total_SIR_bass += abs(SIR_bass[0])\n",
    "    total_SAR_bass += abs(SAR_bass[0])\n",
    "    SDR_drum, ISR_drum, SIR_drum, SAR_drum, perm = mir_eval.separation.bss_eval_images(drums_gt.reshape((1, shape[0],1)), drums.reshape((1, shape[0],1)))\n",
    "    total_SDR_drum += abs(SDR_drum[0])\n",
    "    total_ISR_drum += abs(ISR_drum[0])\n",
    "    total_SIR_drum += abs(SIR_drum[0])\n",
    "    total_SAR_drum += abs(SAR_drum[0])\n",
    "    SDR_vocal, ISR_vocal, SIR_vocal, SAR_vocal, perm = mir_eval.separation.bss_eval_images(vocals_gt.reshape((1, shape[0],1)), vocals.reshape((1, shape[0],1)))\n",
    "    total_SDR_vocal += abs(SDR_vocal[0])\n",
    "    total_ISR_vocal += abs(ISR_vocal[0])\n",
    "    total_SIR_vocal += abs(SIR_vocal[0])\n",
    "    total_SAR_vocal += abs(SAR_vocal[0])\n",
    "    SDR_other, ISR_other, SIR_other, SAR_other, perm = mir_eval.separation.bss_eval_images(others_gt.reshape((1, shape[0],1)), others.reshape((1, shape[0],1)))\n",
    "    total_SDR_other += abs(SDR_other[0])\n",
    "    total_ISR_other += abs(ISR_other[0])\n",
    "    total_SIR_other += abs(SIR_other[0])\n",
    "    total_SAR_other += abs(SAR_other[0])\n",
    "    \n",
    "mean_SDR_bass = total_SDR_bass/len(filenames)\n",
    "mean_ISR_bass = total_ISR_bass/len(filenames)\n",
    "mean_SIR_bass = total_SIR_bass/len(filenames)\n",
    "mean_SAR_bass = total_SAR_bass/len(filenames)\n",
    "mean_SDR_drum = total_SDR_drum/len(filenames)\n",
    "mean_ISR_drum = total_ISR_bass/len(filenames)\n",
    "mean_SIR_drum = total_SIR_drum/len(filenames)\n",
    "mean_SAR_drum = total_SAR_drum/len(filenames)\n",
    "mean_SDR_vocal = total_SDR_vocal/len(filenames)\n",
    "mean_ISR_vocal = total_ISR_bass/len(filenames)\n",
    "mean_SIR_vocal = total_SIR_vocal/len(filenames)\n",
    "mean_SAR_vocal = total_SAR_vocal/len(filenames)\n",
    "mean_SDR_other = total_SDR_other/len(filenames)\n",
    "mean_ISR_other = total_ISR_bass/len(filenames)\n",
    "mean_SIR_other = total_SIR_other/len(filenames)\n",
    "mean_SAR_other = total_SAR_other/len(filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "geological-words",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mean_SDR_bass)\n",
    "print(mean_ISR_bass)\n",
    "print(mean_SIR_bass)\n",
    "print(mean_SAR_bass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "certified-shade",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mean_SDR_drum)\n",
    "print(mean_ISR_drum)\n",
    "print(mean_SIR_drum)\n",
    "print(mean_SAR_drum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "respected-virus",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(mean_SDR_vocal)\n",
    "print(mean_ISR_vocal)\n",
    "print(mean_SIR_vocal)\n",
    "print(mean_SAR_vocal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "young-region",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(mean_SDR_other)\n",
    "print(mean_ISR_other)\n",
    "print(mean_SIR_other)\n",
    "print(mean_SAR_other)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "damaged-account",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = net = ConvNet(inp_size)\n",
    "    # net.load_state_dict(torch.load('Weights/Weights_200_3722932.6015625.pth')) #least score Weights so far\n",
    "net.load_state_dict(torch.load('Weights/Weights_13_95039608.0.pth'))\n",
    "net.eval()\n",
    "mixture, m_phase = load_mixture('./musdb18hq/test/Side Effects Project - Sing With Me/mixture', seq=1)\n",
    "mean = torch.mean(torch.tensor(mixture))\n",
    "std = torch.std(torch.tensor(mixture))\n",
    "inp_n = (torch.tensor(mixture)-mean)/std\n",
    "(time_len, mel_bins) = inp_n.shape\n",
    "inp_n = inp_n.view(1,time_len, mel_bins)\n",
    "bass_mag, vocals_mag, drums_mag,others_mag = net(torch.FloatTensor(inp_n))\n",
    "mask_bass,mask_vocals,mask_drums,mask_others = TimeFreqMasking(bass_mag, vocals_mag, drums_mag, others_mag)\n",
    "drums_mag=torch.FloatTensor(mixture)*np.squeeze(mask_drums)\n",
    "vocals_mag=torch.FloatTensor(mixture)*np.squeeze(mask_vocals)\n",
    "bass_mag=torch.FloatTensor(mixture)*np.squeeze(mask_bass)\n",
    "others_mag=torch.FloatTensor(mixture)*np.squeeze(mask_others)\n",
    "    \n",
    "vocals = vocals_mag.detach().numpy() * m_phase\n",
    "\t#print(vocals.shape)\n",
    "bass = bass_mag.detach().numpy()* m_phase\n",
    "drums = drums_mag.detach().numpy()* m_phase\n",
    "others = others_mag.detach().numpy()* m_phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "detected-screening",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocals_audio = librosa.istft(vocals, win_length=2204,hop_length=1103,window='hann',center='True')\n",
    "bass_audio = librosa.istft(bass, win_length=2204,hop_length=1103,window='hann',center='True')\n",
    "drums_audio = librosa.istft(drums, win_length=2204,hop_length=1103,window='hann',center='True')\n",
    "others_audio = librosa.istft(others, win_length=2204,hop_length=1103,window='hann',center='True')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stuffed-helmet",
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.io.wavfile.write('./musdb18hq/model1/vocal.wav', 44100, vocals_audio)\n",
    "scipy.io.wavfile.write('./musdb18hq/model1/bass.wav', 44100, bass_audio)\n",
    "scipy.io.wavfile.write('./musdb18hq/model1/drums.wav',  44100, drums_audio)\n",
    "scipy.io.wavfile.write('./musdb18hq/model1/other.wav',  44100, others_audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "swedish-model",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nearby-medicare",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
